{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vision Lab","text":""},{"location":"#overview","title":"Overview","text":"<p>Vision Lab is a public template for computer vision deep learning research projects using TorchVision and Lightning AI's PyTorch Lightning.</p> <p>Use Vision Lab to train or finetune the default torchvision Vision Transformer or make it your own by implementing a new model and dataset after cloning the repo.</p> <p>You can fork Vision Lab with the use this template button.</p> <p>[!NOTE] Vision Lab was featured by Weights and Biases in this community spotlight</p>"},{"location":"#source-module","title":"Source Module","text":"<p><code>visionlab.core</code> contains code for the Lightning Module and Trainer.</p> <p><code>visionlab.components</code> contains experiment utilities grouped by purpose for cohesion.</p> <p><code>visionlab.pipeline</code> contains code for data acquistion and preprocessing, and building a TorchDataset and LightningDataModule.</p> <p><code>visionlab.serve</code> contains code for model serving APIs built with FastAPI.</p> <p><code>visionlab.cli</code> contains code for the command line interface built with Typer and Rich.</p> <p><code>visionlab.pages</code> contains code for data apps built with Streamlit.</p> <p><code>visionlab.config</code> assists with project, trainer, and sweep configurations.</p>"},{"location":"#base-requirements-and-extras","title":"Base Requirements and Extras","text":"<p>Vision Lab installs minimal requirements out of the box, and provides extras to make creating robust virtual environments easier. To view the requirements, in setup.cfg, see <code>install_requires</code> for the base requirements and <code>options.extras_require</code> for the available extras.</p> <p>The recommended install is as follows:</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -e \".[all]\"\n</code></pre>"},{"location":"#using-vision-lab","title":"Using Vision Lab","text":"<p>Vision Lab also enables use of a CLI named <code>lab</code> that is built with Typer. This CLI is available in the terminal after install. <code>lab</code>'s features can be viewed with:</p> <pre><code>lab --help\n</code></pre> <p>A fast dev run cab be ran with:</p> <pre><code>lab run dev\n</code></pre> <p>A longer demo run can be inititated with:</p> <pre><code>lab run demo\n</code></pre>"},{"location":"reference/datamodule/","title":"Datamodule","text":""},{"location":"reference/datamodule/#visionlab.CifarDataModule","title":"<code>visionlab.CifarDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A custom LightningDataModule</p>"},{"location":"reference/datamodule/#visionlab.CifarDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>prepares data for the dataloaders</p>"},{"location":"reference/datamodule/#visionlab.CifarDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>used by trainer to setup the dataset for training and evaluation</p>"},{"location":"reference/datamodule/#visionlab.CifarDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>the dataloader used during testing</p>"},{"location":"reference/datamodule/#visionlab.CifarDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>the dataloader used during training</p>"},{"location":"reference/datamodule/#visionlab.CifarDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>the dataloader used during validation</p>"},{"location":"reference/module/","title":"Module","text":""},{"location":"reference/module/#visionlab.VisionTransformer","title":"<code>visionlab.VisionTransformer</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A custom PyTorch Lightning LightningModule for torchvision VisionTransformers</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>str</code> <p>\"Adam\". A valid torch.optim name.</p> <code>'Adam'</code> <code>lr</code> <code>float</code> <p>1e-3</p> <code>0.001</code> <code>accuracy_task</code> <code>str</code> <p>\"multiclass\". One of (binary, multiclass, multilabel).</p> <code>'multiclass'</code> <code>image_size</code> <code>int</code> <p>32</p> <code>32</code> <code>num_classes</code> <code>int</code> <p>100</p> <code>100</code> <code>dropout</code> <code>float</code> <p>0.0</p> <code>0.0</code> <code>attention_dropout</code> <code>float</code> <p>0.0</p> <code>0.0</code> <code>norm_layer</code> <code>Optional[Module]</code> <p>None</p> <code>None</code> <code>conv_stem_configs</code> <code>Optional[List[ConvStemConfig]]</code> <p>None</p> <code>None</code> <code>progress</code> <code>bool</code> <p>False</p> <code>False</code> <code>weights</code> <code>bool</code> <p>False</p> <code>False</code> <code>vit_type</code> <code>str</code> <p>one of (b_16, b_32, l_16, l_32). Default is b_32.</p> <code>'b_32'</code>"},{"location":"reference/module/#visionlab.VisionTransformer.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>configures the <code>torch.optim</code> used in training loop</p>"},{"location":"reference/module/#visionlab.VisionTransformer.forward","title":"<code>forward(x)</code>","text":"<p>calls .forward of a given model flow</p>"},{"location":"reference/module/#visionlab.VisionTransformer.predict_step","title":"<code>predict_step(batch)</code>","text":"<p>returns predicted logits from the trained model</p>"},{"location":"reference/module/#visionlab.VisionTransformer.test_step","title":"<code>test_step(batch, *args)</code>","text":"<p>runs a test step sequence</p>"},{"location":"reference/module/#visionlab.VisionTransformer.training_step","title":"<code>training_step(batch)</code>","text":"<p>runs a training step sequence</p>"},{"location":"reference/module/#visionlab.VisionTransformer.validation_step","title":"<code>validation_step(batch, *args)</code>","text":"<p>runs a validation step sequence</p>"}]}